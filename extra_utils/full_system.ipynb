{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f2a855e",
   "metadata": {},
   "source": [
    "# Aumento de datos utilizando redes generativas para mejorar el reconocimiento y segmentación de imágenes de microscopía óptica.\n",
    "\n",
    "En este notebook yadda yadda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9f3990",
   "metadata": {},
   "source": [
    "## Sección 0: StyleGAN 2\n",
    "Lo primero que se necesita para poner a funcionar este sistema es un checkpoint de la versión vanilla de StyleGAN 2 ([esta](https://github.com/dvschultz/stylegan2)), previamente entrenado para producir imágenes fotorealistas de nematodos. Es necesario convertir el checkpoint a formato pytorch; esto se puede hacer con un script que se puede encontrar en la versión de pytorch de StyleGAN 2 ([esta](https://github.com/rosinality/stylegan2-pytorch)). Un ejemplo de como hacer esta conversión se puede encontrar [aquí](https://github.com/dvschultz/ai/blob/master/Convert_pkl_to_pt.ipynb).\n",
    "\n",
    "Cabe también mencionar que la versión vanilla de StyleGAN usa una versión obsoleta de tensorflow. Afortunadamente, nvidia todavía mantiene compatibilidad por medio de un contenedor de docker (ver https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tensorflow), lo que permite hacer uso de este sistema.\n",
    "\n",
    "Para simplificar la implementación, se asumirá que el checkpoint estará en la dirección .checkpoint/stylegan2/network-snapshot.pt, si se desea usar otra dirección u otro nombre para el checkpoint se deben modificar los files de configuración tool_nema.json, encoder_nema.json y datasetgan_nema.json en el directorio experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7143cdd",
   "metadata": {},
   "source": [
    "## Sección 1: EditGAN\n",
    "\n",
    "### Entrenar el encoder\n",
    "\n",
    "El primer paso es entrenar el encoder. Se asume que las imágenes de entrenamiento tendrán un tamaño de 256x256 y estarán en data/encoder; si se desea usar imágenes con otro tamaño se puede modificar el parámetro im_size en experiments/encoder_nema.json pero es importante notar que la resolución de la imágen tiene que ser consistente para todo el sistema y StyleGAN 2 solo es capaz de generar imágenes cuadradas donde el tamaño de su lado es una potencia de dos.\n",
    "\n",
    "Una importante ventaja de este entrenamiento es que no es necesario utilizar exclusivamente imágenes reales: el propósito del encoder es reproducir imágenes de entrada en el espacio latente de StyleGAN 2 así que para entrenarlo es posible utilizar un conjunto de imágenes sintéticas arbitrariamente grande, producido por el mismo StyleGAN 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b378ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../train_encoder.py --exp experiments/encoder_nema.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315a4905",
   "metadata": {},
   "source": [
    "El comando para continuar con el entrenamiento del encoder a partir de un checkpoint es:\n",
    "\n",
    "<code>  train_encoder.py --exp experiments/encoder_nema.json --resume checkpoint_path.pth  </code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdd24b8",
   "metadata": {},
   "source": [
    "### Preparar un conjunto para datasetGAN\n",
    "\n",
    "El siguiente paso es preparar un pequeño conjunto de parejas imagen-máscara para entrenar a datasetGAN. Este entrenamiento suele requerir de mucha RAM así que se recomiendo utilizar 16 parejas o menos.\n",
    "\n",
    "Las imágenes deben ser nombradas según el formato image_#.png y las máscaras deben ser convertidas a numpy array y deben ser nombradas según el formato image_mask_#.npy; ambos grupos deben ser guardados en el mismo directorio. La siguiente celda se ocupará de tomar un conjunto aleatorio de pares imagen-máscara, de los etiquetados manualmente, y las guardará en el formato correcto.\n",
    "\n",
    "Por default los pares se guardarán en la carpeta extra_utils/data/datasetgan_pairs. Si se deseara cambiar esta dirección sería necesario modificar el config experiments/datasetgan_nema.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcf62c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processor import process_data\n",
    "\n",
    "# Dirección de la carpeta donde se encuentran las imágenes de los nematodos.\n",
    "img_in = '/home/marco/Desktop/proyecto_graduacion/etiquetados/simple_experiment/img_edits'\n",
    "\n",
    "# Dirección de la carpeta donde se encuentran las máscaras.\n",
    "mask_in = '/home/marco/Desktop/proyecto_graduacion/etiquetados/simple_experiment/mask_edits'\n",
    "\n",
    "# Dirección donde guardar los pares imagen-máscara procesados\n",
    "img_out = 'data/datasetgan_pairs'\n",
    "\n",
    "# Cantidad de pares imagen-máscara que muestrear\n",
    "sample_size = 16\n",
    "\n",
    "process_data(img_in, mask_in, img_out, sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9f5e0a",
   "metadata": {},
   "source": [
    "### Introducir conjunto en el espacio latente\n",
    "\n",
    "Lo que sigue es introducir el conjunto que se acaba de crear en el espacio latente de StyleGAN 2. Para esto es necesario que en el siguiente código se reemplace BEST_loss.pth con el nombre del checkpoint del encoder que se va a utilizar. Es necesario hacer este mismo cambio también en el config experiments/tool_nema.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed11f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../python train_encoder.py --exp experiments/encoder_nema.json --resume ../model_encoder/nema/checkpoint/BEST_loss.pth --testing_path data/datasetgan_pairs --latent_sv_folder ../model_encoder/nema/training_embedding --test True "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0cc38c",
   "metadata": {},
   "source": [
    "### Entrenar a DatasetGAN\n",
    "\n",
    "Este es el último entrenamiento de EditGAN, y es el que requiere de mucha RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376003e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../train_interpreter.py --exp experiments/datasetgan_nema.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0a5dd4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> Si el entrenamiento requiere de más RAM de la que se tiene a disposición, igual es posible realizarlo utilizando memoria virtual. Esta es una solución imperfecta y resultará en un entrenamiento tremendamente lento, pero fue lo que permitió realizarlo con 16 pares. Se requiere de un espacio swap suficientemente grande y se realiza con el comando:\n",
    "    \n",
    "<code>  systemd-run --scope -p MemoryMax=26G python train_interpreter.py --exp experiments/datasetgan_nema.json  </code>\n",
    "    \n",
    "Donde MemoryMax es la máxima cantidad de RAM que se le permite usar antes de usar swap.\n",
    "    \n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994b4777",
   "metadata": {},
   "source": [
    "### Generar el conjunto sintético anotado\n",
    "\n",
    "Finalmente, EditGAN está listo para generar el conjunto etiquetado. Se recomienda a este punto generar un conjunto muy grande porque por el momento es de esperarse que salgan muchas máscaras de mala calidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e82bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "\n",
    "import dataset_gen \n",
    "\n",
    "sampling_amount = 128\n",
    "\n",
    "img_out_path = 'data/test_img'\n",
    "\n",
    "mask_out_path = 'data/test_mask'\n",
    "\n",
    "dataset_gen.mode_1(sampling_amount, img_out_path, mask_out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b333f9",
   "metadata": {},
   "source": [
    "Podemos darnos una idea de la calidad del conjunto generado creando un mosácio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fcb41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mosaic_maker import create_mosaic\n",
    "\n",
    "img_in = '/home/marco/Desktop/proyecto_graduacion/editGAN_release/dataset_final/simple/img'\n",
    "\n",
    "mask_in = '/home/marco/Desktop/proyecto_graduacion/editGAN_release/dataset_final/simple/mask'\n",
    "\n",
    "img_num = 64\n",
    "\n",
    "create_mosaic(img_in, mask_in, img_num, img_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0065474c",
   "metadata": {},
   "source": [
    "Es muy probable que a este punto la calidad de la mayoría de las máscaras deje bastante que desear. A continuación se aplicará un filtro a este conjunto recién creado para elegir solamente las mejores máscaras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b4394f",
   "metadata": {},
   "source": [
    "## Sección 2: el filtro\n",
    "\n",
    "El filtro consiste de dos partes:\n",
    "\n",
    "1. Un primer filtrado hecho a partir del tamaño del contorno vermíforme. En el mosáico de arriba debería resultar evidente que las máscaras que se pueden descartar inmediatamente tienden a ser una colección de pequeñas manchas. Un umbral de tamaño mínimo se ocupa de eliminar todas esas y de elegir solamente la forma del nematodo en las que están bien pero visualmente ruidosas.\n",
    "2. Un support vector machine  que intenta determinar cuales máscaras son buenas y cuales malas a partir de los 7 momentos de Hu del contorno del nematodo, de su perímetro y de la magnitud de las 5 componentes de mayor frecuencia de la transformada de fourier de la máscara.\n",
    "\n",
    "Es inevitable a este punto que para entrenar al SVM sea necesario elegir manualmente a un conjunto de máscaras malas y un conjunto de máscaras buenas.\n",
    "\n",
    "El entrenamiento del SVM ser hará con un grid search así que es necesario definir sus parámetros. También digno de nota es que para entrenar el SVM se está usando MinMaxScaler, y ese mismo scaler debe ser utilizado a la hora de hacer inferencia así que se guardará junto con el.\n",
    "\n",
    "NOTA: el grid search está configurado para usar todos los recursos de la computadora que pueda. Si esto no es deseable cambiar la variable n_jobs en train_svm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6716847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from svm_trainer import train_svm\n",
    "\n",
    "# Dirección de las máscaras de buena calidad\n",
    "good_path = '' \n",
    "\n",
    "# Dirección de las máscaras de mala calidad\n",
    "bad_path = ''\n",
    "\n",
    "# Dirección donde guardar el SVM y el scaler\n",
    "save_path = '../.checkpoint/SVM'\n",
    "\n",
    "param_grid = {'C':[1,4,5,6,10],'gamma':[50,85, 90, 100, 110, 115],'kernel':['sigmoid', 'rbf','poly']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3b48f2",
   "metadata": {},
   "source": [
    "Con el SVM entrenado se aplica el filtro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5030c912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_filter import process_image\n",
    "\n",
    "# Este código es un poco obtuso pero fue hecho de \n",
    "# esta manera para paralelizar fácilmente\n",
    "\n",
    "# Dirección donde se encuentran las imágenes por filtrar.\n",
    "img_in = ''\n",
    "\n",
    "# Dirección donde se encuentran las máscaras por filtrar.\n",
    "mask_in = ''\n",
    "\n",
    "# Dirección donde guardar las imágenes que son aceptadas por el filtro. \n",
    "img_out = ''\n",
    "\n",
    "# Dirección donde guardar las máscaras que son aceptadas por el filtro.\n",
    "mask_out = ''\n",
    "\n",
    "# Dirección donde guardar las máscaras que son rechazadas por el filtro. \n",
    "# Si es None, no guarda estas máscaras.\n",
    "bad_out = ''\n",
    "\n",
    "# Dirección del SVM\n",
    "svm_path = '../.checkpoint/SVM'\n",
    "\n",
    "# Dirección del scaler\n",
    "scaler_path = '../.checkpoint/SVM'\n",
    "\n",
    "# Umbral de probabilidad para que el SVM acepte una máscara.\n",
    "svm_proba = 0.8\n",
    "\n",
    "# Umbral de área ocupada por el namatodo. Máscaras con un area inferior serán rechazadas.\n",
    "min_area = 2600\n",
    "\n",
    "masknames = next(walk(mask_in), (None, None, []))[2]  # [] if no file\n",
    "masknames.sort()\n",
    "\n",
    "model = pickle.load(open(svm_path, 'rb'))\n",
    "scaler = pickle.load(open(scaler_path, 'rb'))\n",
    "\n",
    "param = { \n",
    "    'img_in': img_in,\n",
    "    'mask_in': mask_in,\n",
    "    'img_out': img_out,\n",
    "    'mask_out': mask_out,\n",
    "    'bad_out': bad_out,\n",
    "    'min_area': min_area,\n",
    "    'model': model,\n",
    "    'scaler': scaler,\n",
    "    'svm_proba': svm_proba,\n",
    "    'mask_name': ''\n",
    "    }\n",
    "\n",
    "params = []\n",
    "\n",
    "for i in masknames:\n",
    "    temp = param.copy()\n",
    "    temp['mask_name'] = i\n",
    "    params.append(temp)\n",
    "\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    executor.map(process_image, params) \n",
    "\n",
    "\n",
    "t2 = time.perf_counter()\n",
    "time_dif = (t2-t1)/(60*60)\n",
    "\n",
    "print(f'Finished in {time_dif} hours')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df396ad2",
   "metadata": {},
   "source": [
    "Habiendo filtrado los datos, podemos volver a generar un mosáico para ver el conjunto final:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d4b161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mosaic_maker import create_mosaic\n",
    "\n",
    "img_num = 64\n",
    "\n",
    "create_mosaic(img_in, mask_in, img_num, img_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
